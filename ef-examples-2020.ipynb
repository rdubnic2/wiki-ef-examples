{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains example use cases for HTRC's [Extracted Features Dataset](https://wiki.htrc.illinois.edu/display/COM/Extracted+Features+Dataset). This derived dataset, generated from all volumes in HathiTrust, contains volume-level bibliographic metadata, and page-level metadata and features. You can read the full description of the dataset and its fields at the above URL. Given its popularity and the expertise of HTRC staff, we'll be using Python in this Jupyter notebook to demo the use cases, however this dataset can be used with any language, and with or without the libraries used below. This notebook also has [an accompanying web page](https://wiki.htrc.illinois.edu/display/COM/EF+Use+Cases+and+Examples) with more general information about the dataset and how it can be used. If you're new to the dataset, we'd strongly encourage you to start there.\n",
    "\n",
    "Our sample use cases in this notebook are centered around using metadata and features to identify poetry amongst prose. We'll focus on two specific examples:\n",
    "1. Identifying which volume is poetry and which is prose when given mixed volumes by a single author\n",
    "2. Identifying pages of poetry within a single volume that mixes both prose and poetry.\n",
    "\n",
    "As with most Python notebooks, we'll start by importing some libraries we'll need to tackle these examples, [Pandas](https://pandas.pydata.org/), a very common data science library in Python, and the [HTRC FeatureReader](https://github.com/htrc/htrc-feature-reader), a library written by HTRC to ease the use of the Extracted Features Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "from htrc_features import Volume\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "For our first use case, we'll be using two volumes by Ursula K. Le Guin--one volume of prose (*The Left Hand of Darkness*) and one volume of poetry (*Hard Words and Other Poems*). Since the goal of this notebook is to demo a possible use case, and not actually achieve the task of differentiating these two volumes, our results are spoiled, since we know the volume titles. But, in the name of learning, we'll continue as if we are ignorant of the giveaway in the title of one of these volumes!\n",
    "\n",
    "For our second use case, we'll be using one issue of *Harper's Magazine*: volume 142 which includes issues from 1920 and 1921. This particular volume is verified as having poetry within it, including a relatively famous poem by Robert Frost (but probably not the one you're thinking of!).\n",
    "\n",
    "\n",
    "\n",
    "In the interests of not spoiling the results, we'll \n",
    "* **Poetry**\n",
    "    * coo.31924054824473 - Harper's magazine. v.142 1920/21\n",
    "         * \"Fire and Ice\" - Robert Frost - page 67, sequence 79\n",
    "    * Volume 1: mdp.39015000639800 - *Hard words, and other poems* - Ursula K. Le Guin\n",
    "\n",
    "* **Prose:**\n",
    "    * Volume 2: mdp.39015052467530 - *The left hand of darkness* - Ursula K. LeGuin\n",
    "    * coo.31924054824473 - Harper's magazine. v.142 1920/21 (mostly prose!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General workflow:\n",
    "* Show differentiating between volumes of prose and poetry for the same author:\n",
    "    * Use EF files to show higher porportion of capitalized beginLineChars in poetry v. prose (if valid)\n",
    "* Show IDing poetry pages in a volume of mixed poetry and prose\n",
    "    * Find a volume with both, make note of page of poem(s)\n",
    "    * check beginLineChars for pages with higher rates of capitalized beginLineChars\n",
    "    * check for average tokens per page\n",
    "    * if both criteria are met, mark as possible poetry on page\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because these are volumes within a larger work, they have the same basic title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call just one Volume at a time in order to examine its contents. In this example, we are taking the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard words, and other poems / Ursula K. Le Guin. mdp.39015000639800\n",
      "The left hand of darkness / by Ursula K. LeGuin. mdp.39015052467530\n",
      "Harper's magazine. v.142 1920/21 coo.31924054824473\n"
     ]
    }
   ],
   "source": [
    "uklg_v1 = Volume('mdp.39015000639800')\n",
    "# print(uklg_poetry)\n",
    "print(uklg_v1.title, uklg_v1.id)\n",
    "\n",
    "uklg_v2 = Volume('mdp.39015052467530')\n",
    "# print(uklg_prose)\n",
    "print(uklg_v2.title, uklg_v2.id)\n",
    "\n",
    "harpers = Volume('coo.31924054824473')\n",
    "# print(harpers)\n",
    "print(harpers.title, harpers.enumeration_chronology, harpers.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>place</th>\n",
       "      <th>char</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>body</th>\n",
       "      <th>begin</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">body</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">begin</th>\n",
       "      <th>H</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>body</th>\n",
       "      <th>begin</th>\n",
       "      <th>H</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">11</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">body</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">begin</th>\n",
       "      <th>'</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count\n",
       "page section place char       \n",
       "1    body    begin 0         1\n",
       "3    body    begin H         2\n",
       "                   U         1\n",
       "                   a         2\n",
       "9    body    begin H         1\n",
       "11   body    begin '         3\n",
       "                   /         1\n",
       "                   D         1\n",
       "                   H         1\n",
       "                   J         2\n",
       "                   T         9\n",
       "                   W         1\n",
       "                   Z         1\n",
       "                   ^         1\n",
       "                   _         1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uklg_v1_blc = uklg_v1.begin_line_chars()\n",
    "uklg_v2_blc = uklg_v2.begin_line_chars()\n",
    "\n",
    "uklg_v1_blc.head(15)\n",
    "# uklg_v2_blc.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 390 capitalized begin line characters\n",
      "Found 389 lowercase begin line characters\n",
      "Found 779 total begin line characters\n",
      "Uppercase characters make up 50.06% of begin line characters\n"
     ]
    }
   ],
   "source": [
    "uklg_v1_blc_caps = 0\n",
    "uklg_v1_blc_lower = 0\n",
    "\n",
    "for idx, count in uklg_v1_blc.iterrows():\n",
    "    # print(idx)\n",
    "    # print(count)\n",
    "    blc = idx[3]\n",
    "    # print(blc)\n",
    "    if blc.isupper() == True:\n",
    "        uklg_v1_blc_caps +=1\n",
    "    else:\n",
    "        uklg_v1_blc_lower +=1\n",
    "\n",
    "print(f\"Found {uklg_v1_blc_caps} capitalized begin line characters\")\n",
    "print(f\"Found {uklg_v1_blc_lower} lowercase begin line characters\")\n",
    "print(f\"Found {uklg_v1_blc_caps + uklg_v1_blc_lower} total begin line characters\")\n",
    "\n",
    "uklg_v1_caps_pct = uklg_v1_blc_caps / len(uklg_v1_blc['count'])\n",
    "print(f\"Uppercase characters make up {'{:.2%}'.format(uklg_v1_caps_pct)} of begin line characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1081 capitalized begin line characters\n",
      "Found 3520 lowercase begin line characters\n",
      "Found 4601 total begin line characters\n",
      "Uppercase characters make up 23.49% of begin line characters\n"
     ]
    }
   ],
   "source": [
    "uklg_v2_blc_caps = 0\n",
    "uklg_v2_blc_lower = 0\n",
    "\n",
    "for idx, count in uklg_v2_blc.iterrows():\n",
    "    # print(count)\n",
    "    blc = idx[3]\n",
    "    # print(blc)\n",
    "    if blc.isupper() == True:\n",
    "        uklg_v2_blc_caps +=1\n",
    "    else:\n",
    "        uklg_v2_blc_lower +=1\n",
    "\n",
    "print(f\"Found {uklg_v2_blc_caps} capitalized begin line characters\")\n",
    "print(f\"Found {uklg_v2_blc_lower} lowercase begin line characters\")\n",
    "print(f\"Found {uklg_v2_blc_caps + uklg_v2_blc_lower} total begin line characters\")\n",
    "        \n",
    "        \n",
    "uklg_v2_caps_pct = uklg_prose_blc_caps / (uklg_v2_blc_caps + uklg_v2_blc_lower)\n",
    "print(f\"Uppercase characters make up {'{:.2%}'.format(uklg_v2_caps_pct)} of begin line characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the two sets of results, it seems highly likely that our first volume, where just over 50% of our begin line characters are capitalized, is poetry while the second volume, with begin line characters half as likely to be capitalized, is likely to be prose. Though this is a simple and broad method, it is was relatively quick to write and run, which could make it a generalizable process for exploring a workset or collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding poetry within a mixed volume\n",
    "We've shown above that, given two volumes from a single author, we can use EF files--specifically the page-level metadata--to identify which is poetry and which is prose. But what about a single volume with mixed poetry and prose included? A similar workflow should be successful, but let's try.\n",
    "\n",
    "First, we'll replicate the begin line characters analysis, as before, but modified slightly to delineate pages that are suspected poetry from suspected prose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `harpers_tokens.mean` not found.\n"
     ]
    }
   ],
   "source": [
    "harpers_tokens.mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens in this volume: 619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "page\n",
       "1       0\n",
       "2      44\n",
       "3     502\n",
       "4       0\n",
       "5      25\n",
       "6     666\n",
       "7     752\n",
       "8     155\n",
       "9       0\n",
       "10     18\n",
       "11    502\n",
       "12    780\n",
       "13    200\n",
       "14    756\n",
       "15     11\n",
       "Name: tokenCount, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tokens = harpers.tokens_per_page()\n",
    "\n",
    "h_avg_token_ct = h_tokens.mean()\n",
    "print(f\"Average tokens in this volume: {round(h_avg_token_ct)}\")\n",
    "\n",
    "h_tokens.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376.95478765907876\n"
     ]
    }
   ],
   "source": [
    "h_tokens.describe()\n",
    "h_tokens.std()\n",
    "\n",
    "benchmark = (h_avg_token_ct - h_tokens.std())\n",
    "\n",
    "print(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holder for some text explaining above and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 suspected pages of poetry\n",
      "Found 843 suspected pages of prose\n",
      "[16, 18, 35, 37, 39, 40, 69, 70, 71, 72, 73, 74, 75, 76, 79, 81, 82, 101, 105, 108, 141, 147, 148, 156, 157, 171, 173, 175, 180, 182, 183, 184, 185, 187, 188, 189, 190, 242, 251, 253, 276, 277, 293, 297, 301, 307, 337, 339, 341, 342, 344, 350, 357, 361, 363, 365, 368, 371, 373, 375, 377, 379, 381, 383, 389, 411, 413, 415, 416, 428, 435, 437, 459, 463, 493, 494, 495, 497, 498, 551, 553, 554, 559, 560, 563, 564, 582, 583, 586, 587, 588, 615, 616, 617, 618, 620, 652, 656, 659, 660, 691, 699, 700, 702, 731, 734, 753, 757, 758, 777, 781, 782, 827, 829, 830, 844, 882, 884, 886, 887, 888, 890, 891, 894, 895, 904, 912, 915, 919, 925, 931, 933, 935, 938, 939, 943, 951, 955, 958, 959, 965, 966, 967, 969, 972, 974, 977]\n"
     ]
    }
   ],
   "source": [
    "poetry_pages = []\n",
    "prose_pages = []\n",
    "benchmark = (h_avg_token_ct - h_tokens.std())\n",
    "\n",
    "for page, count in h_tokens.iteritems():\n",
    "    # print(page)\n",
    "    # print(count)\n",
    "    if (benchmark - 100) <= count <= (benchmark + 100):\n",
    "        # print(f\"YOWIE! Page {page} has {count} tokens!\")\n",
    "        poetry_pages.append(page)\n",
    "    else:\n",
    "        prose_pages.append(page)\n",
    "        \n",
    "print(f\"Found {len(poetry_pages)} suspected pages of poetry\")\n",
    "print(f\"Found {len(prose_pages)} suspected pages of prose\")\n",
    "\n",
    "print(poetry_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above sets a first pass list of possible pages that contain poetry/prose and added the page numbers to the appropriate list. We've identified these pages by first defining a benchmark that could indicate the presence of poetry: where the total tokens on a page are within 100 of a standard deviation of tokens per page for this volume of *Harper's Magazine*. This is a bit of an estimate, and is (and should) only be used as a first-pass method for weeding out pages that are very unlikely to contain traditionally-structured poetry (those with large token counts). However, this method alone is not useful, so we must combine it with further analysis to be sure of results. We'll do this by looking at the percentage of begin-line characters (letters) are capitalized, and see if we can find a trend.\n",
    "\n",
    "We'll be re-using the general code and workflow from the Le Guin example, with some modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>place</th>\n",
       "      <th>char</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page section  place char  count\n",
       "0     2    body  begin    (      1\n",
       "1     2    body  begin    B      1\n",
       "2     2    body  begin    H      1\n",
       "3     2    body  begin    I      1\n",
       "4     2    body  begin    J      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_blc = harpers.begin_line_chars()\n",
    "h_blc = h_blc.reset_index() # flattening the dataframe for a bit more clarity in code\n",
    "\n",
    "h_blc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holder for some text about above and below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 977 uppercase BLC\n",
      "Found 956 lowercase BLC\n",
      "Found 871 non-alpha BLC\n"
     ]
    }
   ],
   "source": [
    "h_blc_up = {}\n",
    "h_blc_low = {}\n",
    "h_blc_nonalpha = {}\n",
    "\n",
    "for i, r in h_blc.iterrows():\n",
    "    page = r[0]\n",
    "    section = r[1]\n",
    "    token_place = r[2]\n",
    "    char = r[3]\n",
    "#     print(char)\n",
    "    count = r[4]\n",
    "#     print(count)\n",
    "    \n",
    "    if char.isalpha() == True:\n",
    "        if char.isupper() == True:\n",
    "            if page not in h_blc_up:\n",
    "                h_blc_up[page] = 1\n",
    "            else:\n",
    "                h_blc_up[page] += 1\n",
    "    \n",
    "        else:\n",
    "            if page not in h_blc_low:\n",
    "                h_blc_low[page] = 1\n",
    "            else:\n",
    "                h_blc_low[page] += 1\n",
    "    else:\n",
    "        if page not in h_blc_nonalpha:\n",
    "            h_blc_nonalpha[page] = 1\n",
    "        else:\n",
    "            h_blc_nonalpha[page] += 1\n",
    "\n",
    "print(f\"Found {len(h_blc_up)} uppercase BLC\")\n",
    "print(f\"Found {len(h_blc_low)} lowercase BLC\")\n",
    "print(f\"Found {len(h_blc_nonalpha)} non-alpha BLC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with three different dictionaries with common keys--sequence/page number--we'll create a new DataFrame with each page sequence as a row, and values for each type of begin line character as columns. To do this,  create a DataFrame from the dictionaries, rename the columns, transpose its axes to move pages to rows, and then fill all `NaN` values (which are a result from pages with no lower/upper/non-alpha begin line characters) with 0 in order to compare them numerically to the other values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non-alpha</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>uppercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    non-alpha  lowercase  uppercase\n",
       "2         1.0        0.0        5.0\n",
       "3        11.0       15.0        7.0\n",
       "5         2.0        0.0        5.0\n",
       "6         7.0        9.0       16.0\n",
       "7         8.0        6.0       21.0\n",
       "8         1.0        1.0       11.0\n",
       "10        1.0        0.0        2.0\n",
       "11        0.0       15.0       13.0\n",
       "12        2.0       21.0        9.0\n",
       "13        0.0       14.0        3.0\n",
       "14        2.0       19.0       12.0\n",
       "15        0.0        0.0        2.0\n",
       "16        4.0       18.0       11.0\n",
       "17        0.0        0.0        2.0\n",
       "18        2.0       14.0        8.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaries = [h_blc_nonalpha, h_blc_low, h_blc_up]\n",
    "harpers_blc_df = pd.DataFrame.from_dict(dictionaries)\n",
    "# harpers_blc_df.head(15)\n",
    "\n",
    "harpers_blc_df = harpers_blc_df.rename(index={0: \"non-alpha\", 1: \"lowercase\", 2: \"uppercase\"})\n",
    "harpers_blc_df = harpers_blc_df.T\n",
    "harpers_blc_df = harpers_blc_df.fillna(0)\n",
    "harpers_blc_df.head(15)\n",
    "# harpers_blc_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this flattened DataFrame, we can now go through the rows and track which category of begin line characters is the most common on each page, and then add the page number to lists to track each page. For this example, since we're only interested in pages of possible poetry, we're simply continuing the loop if a page has more lowercase or non-alpha begin line characters, and only tracking pages where uppercase characters are most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 6, 7, 8, 10, 15, 17, 79, 80, 81, 82, 231, 237, 242]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blc_poss_poetry = []\n",
    "\n",
    "for i, r in harpers_blc_df.iterrows():\n",
    "#     print(i)\n",
    "    non_alpha = r[0]\n",
    "    low = r[1]\n",
    "    up = r[2]\n",
    "    total = non_alpha + low + up\n",
    "#     print(total)\n",
    "    if non_alpha > low and non_alpha > up:\n",
    "#         print('non-alpha')\n",
    "        continue\n",
    "        \n",
    "    elif low > non_alpha and low > up:\n",
    "#         print('low')\n",
    "        continue\n",
    "        \n",
    "    elif up > non_alpha and up > low:\n",
    "        blc_poss_poetry.append(i)\n",
    "\n",
    "blc_poss_poetry[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two lists of potential pages, one based on token counts by page (`poetry_pages`) and one based on capital letters making up the plurality of begin line characters (`blc_poss_poetry`)--we can identify pages that appear in both lists, and thus are the most likely candidates to be poetry instead of prose. We can do this using one simple piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{79,\n",
       " 81,\n",
       " 82,\n",
       " 242,\n",
       " 307,\n",
       " 350,\n",
       " 416,\n",
       " 691,\n",
       " 884,\n",
       " 890,\n",
       " 895,\n",
       " 943,\n",
       " 955,\n",
       " 958,\n",
       " 966,\n",
       " 969,\n",
       " 977}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry = set(poetry_pages).intersection(blc_poss_poetry)\n",
    "\n",
    "poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 990 pages, we now have 17 strong candidates for pages that contain poetry! However, it's important to remember that this workflow is part of the exploratory process, so we should manually check our results, and, if applicable, improve our process and code to maximize accuracy given what we know about our data. Since this issue of *Harper's Magazine* is in the public domain, we can check our result pages and see what we find. To make this easy for use in our Jupyter notebook, we'll use the URL to the first page of the volume in HathiTrust's page turner as a base, and then substitute the sequence number at the very end with our possible pages in order to print URLs to each page (or see an alternative in the code for opening each page in a browser tab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=79\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=81\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=82\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=242\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=307\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=350\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=416\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=691\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=884\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=890\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=895\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=943\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=955\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=958\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=966\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=969\n",
      "https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq=977\n"
     ]
    }
   ],
   "source": [
    "# import webbrowser # OPTIONAL: import a library we can use use to open each page URLs in a new browser tab\n",
    "\n",
    "harpers_url = 'https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq='\n",
    "\n",
    "\n",
    "for page in sorted(poetry):\n",
    "    page_url = harpers_url + str(page)\n",
    "    print(page_url)\n",
    "    # webbrowser.open_new_tab(page_url) # OPTIONAL: this code will open each page URL in a new browser \n",
    "                                        # tab, for investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
