{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the Python modules we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "from htrc_features import utils\n",
    "from htrc_features import Volume\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data being used:\n",
    "* **Poetry**\n",
    "    * coo.31924054824473 - Harper's magazine. v.142 1920/21\n",
    "         * \"Fire and Ice\" - Robert Frost - page 67, sequence 79\n",
    "    * uc1.32106006795980 - *The last song : poems* - Joy Harjo\n",
    "    * mdp.39015000639800 - *Hard words, and other poems* - Ursula K. Le Guin\n",
    "\n",
    "* **Prose:**\n",
    "    * mdp.39015037761049 - *The spiral of memory : interviews* - Joy Harjo; edited by Laura Coltelli\n",
    "    * mdp.39015052467530 - *The left hand of darkness* - Ursula K. LeGuin\n",
    "    * Rest of that issue of Harper's Magazine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General workflow:\n",
    "* Show differentiating between volumes of prose and poetry for the same author:\n",
    "    * Use EF files to show higher porportion of capitalized beginLineChars in poetry v. prose (if valid)\n",
    "* Show IDing poetry pages in a volume of mixed poetry and prose\n",
    "    * Find a volume with both, make note of page of poem(s)\n",
    "    * check beginLineChars for pages with higher rates of capitalized beginLineChars\n",
    "    * check for average tokens per page\n",
    "    * if both criteria are met, mark as possible poetry on page\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['uc1.32106006795980', 'mdp.39015000639800']\n",
      "None\n",
      "['mdp.39015037761049', 'mdp.39015052467530']\n",
      "None\n",
      "['coo.31924054824473']\n",
      "None\n",
      "['mdp.39015050507618', 'mdp.39015046463629', 'mdp.39015054095784']\n"
     ]
    }
   ],
   "source": [
    "fr_poetry = FeatureReader(ids=['uc1.32106006795980','mdp.39015000639800'])\n",
    "fr_prose = FeatureReader(ids=['mdp.39015037761049','mdp.39015052467530'])\n",
    "fr_harpers = FeatureReader(ids=['coo.31924054824473'])\n",
    "\n",
    "fr_got = FeatureReader(ids=['mdp.39015050507618','mdp.39015046463629','mdp.39015054095784'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because these are volumes within a larger work, they have the same basic title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last song : [poems] / Joy Harjo.uc1.32106006795980\n",
      "Hard words, and other poems / Ursula K. Le Guin.mdp.39015000639800\n",
      "The spiral of memory : interviews / Joy Harjo ; edited by Laura Coltelli.mdp.39015037761049\n",
      "The left hand of darkness / by Ursula K. LeGuin.mdp.39015052467530\n",
      "Harper's magazine. v.142 1920/21 coo.31924054824473\n"
     ]
    }
   ],
   "source": [
    "for vol in fr_poetry.volumes():\n",
    "    print(vol.title + vol.id)\n",
    "    \n",
    "for vol in fr_prose.volumes():\n",
    "    print(vol.title + vol.id)\n",
    "    \n",
    "for vol in fr_harpers.volumes():\n",
    "    print(vol.title, vol.enumeration_chronology, vol.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and page structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call just one Volume at a time in order to examine its contents. In this example, we are taking the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Volume: Hard words, and other poems /... (1981) by Le Guin, Ursula K. 1929->\n",
      "<Volume: The left hand of darkness / by... (1969) by Le Guin, Ursula K. 1929->\n",
      "<Volume: Harper's magazine. (1921) by Allen, Frederick Lewis 1890-1954 ed.>\n",
      "<Volume: The last song : [poems] / Joy... (1975) by Harjo, Joy.>\n",
      "<Volume: The spiral of memory : intervi... (1996) by Harjo, Joy.>\n"
     ]
    }
   ],
   "source": [
    "uklg_poetry = Volume('mdp.39015000639800')\n",
    "print(uklg_poetry)\n",
    "\n",
    "uklg_prose = Volume('mdp.39015052467530')\n",
    "print(uklg_prose)\n",
    "\n",
    "harpers = Volume('coo.31924054824473')\n",
    "print(harpers)\n",
    "\n",
    "jh_poetry = Volume('uc1.32106006795980')\n",
    "print(jh_poetry)\n",
    "\n",
    "jh_prose = Volume('mdp.39015037761049')\n",
    "print(jh_prose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "uklg_poetry_blc = uklg_poetry.begin_line_chars()\n",
    "# uklg_poetry_blc.head(25)\n",
    "\n",
    "harpers_blc = harpers.begin_line_chars()\n",
    "# harpers_blc.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 390 capitalized begin line characters\n",
      "Found 389 lowercase begin line characters\n",
      "Found 779 total begin line characters\n",
      "Uppercase characters make up 50.06% of begin line characters\n"
     ]
    }
   ],
   "source": [
    "uklg_blc_caps = 0\n",
    "uklg_blc_lower = 0\n",
    "\n",
    "for char, counts in uklg_poetry.begin_line_chars().iterrows():\n",
    "    # print(char)\n",
    "    blc = char[3]\n",
    "    # print(blc)\n",
    "    if blc.isupper() == True:\n",
    "        uklg_blc_caps +=1\n",
    "    else:\n",
    "        uklg_blc_lower +=1\n",
    "\n",
    "print(f\"Found {uklg_blc_caps} capitalized begin line characters\")\n",
    "print(f\"Found {uklg_blc_lower} lowercase begin line characters\")\n",
    "print(f\"Found {uklg_blc_caps + uklg_blc_lower} total begin line characters\")\n",
    "\n",
    "uklg_caps_pct = uklg_blc_caps / len(uklg_poetry_blc['count'])\n",
    "print(f\"Uppercase characters make up {'{:.2%}'.format(uklg_caps_pct)} of begin line characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1081 capitalized begin line characters\n",
      "Found 3520 lowercase begin line characters\n",
      "Found 4601 total begin line characters\n",
      "Uppercase characters make up 23.49% of begin line characters\n"
     ]
    }
   ],
   "source": [
    "uklg_prose_blc_caps = 0\n",
    "uklg_prose_blc_lower = 0\n",
    "\n",
    "for char, counts in uklg_prose.begin_line_chars().iterrows():\n",
    "    # print(char)\n",
    "    blc = char[3]\n",
    "    # print(blc)\n",
    "    if blc.isupper() == True:\n",
    "        uklg_prose_blc_caps +=1\n",
    "    else:\n",
    "        uklg_prose_blc_lower +=1\n",
    "\n",
    "print(f\"Found {uklg_prose_blc_caps} capitalized begin line characters\")\n",
    "print(f\"Found {uklg_prose_blc_lower} lowercase begin line characters\")\n",
    "print(f\"Found {uklg_prose_blc_caps + uklg_prose_blc_lower} total begin line characters\")\n",
    "        \n",
    "        \n",
    "uklg_prose_caps_pct = uklg_prose_blc_caps / (uklg_prose_blc_caps + uklg_prose_blc_lower)\n",
    "print(f\"Uppercase characters make up {'{:.2%}'.format(uklg_prose_caps_pct)} of begin line characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the two sets of results, it seems highly likely that our first volume, where just over 50% of our begin line characters are capitalized, is poetry while the second volume, with begin line characters half as likely to be capitalized, is likely to be prose. Though this is a simple and broad method, it is generalizable and relatively quick for both human and compute time, which could make it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding poetry within a mixed volume\n",
    "We've shown above that, given two volumes from a single author, we can use EF files--specifically the page-level metadata--to identify which is poetry and which is prose. But what about a single volume with mixed poetry and prose included? A similar workflow should be successful, but let's try.\n",
    "\n",
    "First, we'll replicate the begin line characters analysis, as before, but modified slightly to delineate pages that are suspected poetry from suspected prose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "harpers_tokens.mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens in this volume: 619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "page\n",
       "1       0\n",
       "2      44\n",
       "3     502\n",
       "4       0\n",
       "5      25\n",
       "6     666\n",
       "7     752\n",
       "8     155\n",
       "9       0\n",
       "10     18\n",
       "11    502\n",
       "12    780\n",
       "13    200\n",
       "14    756\n",
       "15     11\n",
       "Name: tokenCount, dtype: int64"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tokens = harpers.tokens_per_page()\n",
    "\n",
    "h_avg_token_ct = h_tokens.mean()\n",
    "print(f\"Average tokens in this volume: {round(h_avg_tokens)}\")\n",
    "\n",
    "h_tokens.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     990.000000\n",
       "mean      619.421212\n",
       "std       242.466424\n",
       "min         0.000000\n",
       "25%       462.000000\n",
       "50%       732.500000\n",
       "75%       793.000000\n",
       "max      1398.000000\n",
       "Name: tokenCount, dtype: float64"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tokens.describe()\n",
    "h_tokens.std()\n",
    "\n",
    "benchmark = (h_avg_token_ct - h_tokens.std())\n",
    "\n",
    "print(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holder for some text explaining above and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 suspected pages of poetry\n",
      "Found 843 suspected pages of prose\n",
      "[16, 18, 35, 37, 39, 40, 69, 70, 71, 72, 73, 74, 75, 76, 79, 81, 82, 101, 105, 108, 141, 147, 148, 156, 157, 171, 173, 175, 180, 182, 183, 184, 185, 187, 188, 189, 190, 242, 251, 253, 276, 277, 293, 297, 301, 307, 337, 339, 341, 342, 344, 350, 357, 361, 363, 365, 368, 371, 373, 375, 377, 379, 381, 383, 389, 411, 413, 415, 416, 428, 435, 437, 459, 463, 493, 494, 495, 497, 498, 551, 553, 554, 559, 560, 563, 564, 582, 583, 586, 587, 588, 615, 616, 617, 618, 620, 652, 656, 659, 660, 691, 699, 700, 702, 731, 734, 753, 757, 758, 777, 781, 782, 827, 829, 830, 844, 882, 884, 886, 887, 888, 890, 891, 894, 895, 904, 912, 915, 919, 925, 931, 933, 935, 938, 939, 943, 951, 955, 958, 959, 965, 966, 967, 969, 972, 974, 977]\n"
     ]
    }
   ],
   "source": [
    "poetry_pages = []\n",
    "prose_pages = []\n",
    "benchmark = (h_avg_token_ct - h_tokens.std())\n",
    "\n",
    "for page, count in harpers_tokens.iteritems():\n",
    "    # print(page)\n",
    "    # print(count)\n",
    "    if (benchmark - 100) <= count <= (benchmark + 100):\n",
    "        # print(f\"YOWIE! Page {page} has {count} tokens!\")\n",
    "        poetry_pages.append(page)\n",
    "    else:\n",
    "        prose_pages.append(page)\n",
    "        \n",
    "print(f\"Found {len(poetry_pages)} suspected pages of poetry\")\n",
    "print(f\"Found {len(prose_pages)} suspected pages of prose\")\n",
    "\n",
    "print(poetry_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above sets a first pass list of possible pages that contain poetry/prose and added the page numbers to the appropriate list. We've identified these pages by first defining a benchmark that could indicate the presence of poetry: where the total tokens on a page are within 100 of a standard deviation of tokens per page for this volume of *Harper's Magazine*. This is a bit of an estimate, and is (and should) only be used as a first-pass method for weeding out pages that are very unlikely to contain traditionally-structured poetry (those with large token counts). However, this method alone is not useful, so we must combine it with further analysis to be sure of results. We'll do this by looking at the percentage of begin-line characters (letters) are capitalized, and see if we can find a trend.\n",
    "\n",
    "We'll be re-using the general code and workflow from the Le Guin example, with some modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>place</th>\n",
       "      <th>char</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>body</td>\n",
       "      <td>begin</td>\n",
       "      <td>J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page section  place char  count\n",
       "0     2    body  begin    (      1\n",
       "1     2    body  begin    B      1\n",
       "2     2    body  begin    H      1\n",
       "3     2    body  begin    I      1\n",
       "4     2    body  begin    J      1"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_blc = harpers.begin_line_chars()\n",
    "h_blc = h_blc.reset_index() # flattening the dataframe for a bit more clarity in code\n",
    "\n",
    "h_blc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holder for some text about above and below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h_blc_up = {}\n",
    "h_blc_low = {}\n",
    "h_blc_nonalpha = {}\n",
    "\n",
    "for i, r in h_blc.iterrows():\n",
    "    page = r[0]\n",
    "    section = r[1]\n",
    "    token_place = r[2]\n",
    "    char = r[3]\n",
    "#     print(char)\n",
    "    count = r[4]\n",
    "#     print(count)\n",
    "    \n",
    "    if char.isalpha() == True:\n",
    "        if char.isupper() == True:\n",
    "            if page not in h_blc_up:\n",
    "                h_blc_up[page] = 1\n",
    "            else:\n",
    "                h_blc_up[page] += 1\n",
    "    \n",
    "        else:\n",
    "            if page not in h_blc_low:\n",
    "                h_blc_low[page] = 1\n",
    "            else:\n",
    "                h_blc_low[page] += 1\n",
    "    else:\n",
    "        if page not in h_blc_nonalpha:\n",
    "            h_blc_nonalpha[page] = 1\n",
    "        else:\n",
    "            h_blc_nonalpha[page] += 1\n",
    "\n",
    "# for r in h_blc.index:\n",
    "#     page = r[i]\n",
    "#     print(r)\n",
    "#   print(f\"this is r: {r}\")\n",
    "\n",
    "#     print(r.values[i])\n",
    "#   print(type(r[3]))\n",
    "# h_blc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# verifying results\n",
    "\n",
    "for k, v in h_blc_up.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holder text to explain above and below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non-alpha</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>uppercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    non-alpha  lowercase  uppercase\n",
       "2         1.0        0.0        5.0\n",
       "3        11.0       15.0        7.0\n",
       "5         2.0        0.0        5.0\n",
       "6         7.0        9.0       16.0\n",
       "7         8.0        6.0       21.0\n",
       "8         1.0        1.0       11.0\n",
       "10        1.0        0.0        2.0\n",
       "11        0.0       15.0       13.0\n",
       "12        2.0       21.0        9.0\n",
       "13        0.0       14.0        3.0\n",
       "14        2.0       19.0       12.0\n",
       "15        0.0        0.0        2.0\n",
       "16        4.0       18.0       11.0\n",
       "17        0.0        0.0        2.0\n",
       "18        2.0       14.0        8.0\n",
       "19        3.0       18.0       11.0\n",
       "20        0.0       21.0        7.0\n",
       "21        0.0       20.0       13.0\n",
       "22        2.0       21.0       10.0\n",
       "23        2.0       20.0       11.0\n",
       "24        2.0       19.0       12.0\n",
       "25        1.0       20.0        8.0\n",
       "26        1.0       19.0       10.0\n",
       "27        1.0       18.0       14.0\n",
       "28        2.0       19.0        9.0"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaries = [h_blc_nonalpha, h_blc_low, h_blc_up]\n",
    "harpers_blc_df = pd.DataFrame.from_dict(dictionaries)\n",
    "# harpers_blc_df.head(15)\n",
    "\n",
    "harpers_blc_df = harpers_blc_df.rename(index={0: \"non-alpha\", 1: \"lowercase\", 2: \"uppercase\"})\n",
    "harpers_blc_df = harpers_blc_df.T\n",
    "harpers_blc_df = harpers_blc_df.fillna(0)\n",
    "harpers_blc_df.head(15)\n",
    "# harpers_blc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "blc_poss_poetry = []\n",
    "\n",
    "for i, r in harpers_blc_df.iterrows():\n",
    "#     print(i)\n",
    "    non_alpha = r[0]\n",
    "    low = r[1]\n",
    "    up = r[2]\n",
    "    total = non_alpha + low + up\n",
    "#     print(total)\n",
    "    if non_alpha > low and non_alpha > up:\n",
    "#         print('non-alpha')\n",
    "        continue\n",
    "        \n",
    "    elif low > non_alpha and low > up:\n",
    "#         print('low')\n",
    "        continue\n",
    "        \n",
    "    elif up > non_alpha and up > low:\n",
    "        blc_poss_poetry.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verifying results\n",
    "\n",
    "blc_poss_poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two lists of potential pages, one based on token counts by page (`poetry_pages`) and one based on capital letters making up the plurality of begin line characters (`blc_poss_poetry`)--we can identify pages that appear in both lists, and thus are the most likely candidates to be poetry instead of prose. We can do this using one simple piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{79,\n",
       " 81,\n",
       " 82,\n",
       " 242,\n",
       " 307,\n",
       " 350,\n",
       " 416,\n",
       " 691,\n",
       " 884,\n",
       " 890,\n",
       " 895,\n",
       " 943,\n",
       " 955,\n",
       " 958,\n",
       " 966,\n",
       " 969,\n",
       " 977}"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry = set(poetry_pages).intersection(blc_poss_poetry)\n",
    "\n",
    "poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that this workflow is part of the exploratory process, so we should manually check the above the results, and then refine our process to make sure it's as accurate as possible based on our data. Since this issue of *Harper's Magazine* is in the public domain, we can check the above pages and see what we find. To make this easy for use in our Jupyter notebook, we'll use a the URL to the first page of the volume in HathiTrust's page turner and then substitute the sequence number at the very end with our possible pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser # optional library to use which will open each of our page URLs in a new browser tab\n",
    "\n",
    "harpers_url = 'https://babel.hathitrust.org/cgi/pt?id=coo.31924054824473&view=1up&seq='\n",
    "\n",
    "\n",
    "for page in sorted(poetry):\n",
    "    page_url = harpers_url + str(page)\n",
    "#     print(page_url) # instead of opening tabs, we could just print the URLs and open them manually\n",
    "    webbrowser.open_new_tab(page_url) # optional: will open each page URL in a new browser tab, for investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
