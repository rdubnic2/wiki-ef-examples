{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with HTRC Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will get you up-and-running with the HTRC Extracted Features dataset. Learn more about the data: https://wiki.htrc.illinois.edu/x/GoA5Ag\n",
    "\n",
    "The code and instructions used in this notebook combine elements from a Programming Historian lesson called \"Text Mining in Python through the HTRC Feature Reader\" (https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) and the Berkeley Data Science Module, \"Library-HTRC\" (https://github.com/ds-modules/Library-HTRC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and reading in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the Python modules we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import pandas \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted Featuers files are originally formatted in JSON notation and compressed; you'll notice the file format is '.json.bz2'. The FeatureReader library is able to work with the files in that format with needing to decompress the files.\n",
    "\n",
    "Within the library, there is a **FeatureReader object** that is used for loading the dataset files and making sense of them. It returns a **Volume object** for each file. A Volume is a representation of a single item in HathiTrust, for example a book or other textual work. From the Volume, you can access features about the work. To drill down to the features derived from individual pages, use the **Page object**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to get the FeatureReader ready to use by pointing it to the file paths for the sample Extracted Features files we are using in this notebook. The files are in directory called 'ef-data'. We'll be using the Extracted Features files four our workset of all 30 volumes of Josiah Conder's _The Modern Traveller_.\n",
    " \n",
    "With fr = FeatureReader(paths) below, the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of file paths from ef-data directory and load the data in the Feature Reader\n",
    "\n",
    "file_paths = glob.glob('ef-data/*.bz2') # glob is a library that will search a file path and return \n",
    "                                        # files with a given extension (format)\n",
    "file_paths.sort(reverse=True)\n",
    "\n",
    "print(file_paths)\n",
    "    \n",
    "fr_mt = FeatureReader(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because these are volumes within a larger work, they have the same basic title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vol in fr_mt.volumes():\n",
    "    print(vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and page structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call just one Volume at a time in order to examine its contents. In this example, we are taking the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol = fr_mt.first()\n",
    "vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_mt.volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the URL for the volume and CTRL-click it to find the corresponding item in the HathiTrust Digital Library (HTDL). \n",
    "\n",
    "These volumes are in the public domain, so we will find that they are available for \"Full View\" in the HTDL. If they were still under copyright, we would be taken to a \"Limited View\" page. The Extracted Features dataset includes a snapshot of 15.7 million volumes from the HTDL and is agnostic to rights status, as the files represent data about the volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what other metadata elements are available to you for each item in its corresponding Extracted Features file. Put your cursor between the period and the end parenthesis, and press tab. You can choose from the dropdown list. Then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your cursor between the period . and the end parenthesis ) and press tab. You can choose from the dropdown list.\n",
    "print(vol.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to access the first features of vol, which is a table of total words for every single page. These can be accessed simply by calling vol.tokens_per_page()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokens_per_page()\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. \n",
    "\n",
    "With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a plot method for data graphics. Without extra arguments, tokens.plot() will assume that you want a line chart with the page on the x-axis and word count on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get here? When we ran vol.tokens_per_page(), it returned a Pandas DataFrame. This means that after setting tokens, we're no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas. tokens.head() used a DataFrame method to look at the first few rows of the dataset, and tokens.plot() uses a method from Pandas to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Token List\n",
    "Another DataFrame accessible to us is `vol.tokenlist()`, which can be called to return section-, part-of-speech-, and word-specific details.\n",
    "\n",
    "Let's use this method to look at some words deeper into the book: from 1000th to 1100th row, skipping by 15, denoted by `[1000:1100:15]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()\n",
    "tl[1000:1100:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the data is returned as a Pandas DataFrame. This time, there is much more information. The columns in bold are an index. Unlike the typical one-dimensional index seen before, here there are four dimensions to the index: page, section, token, and pos. This row says that for the 24th page, in the body section (i.e. ignoring any words in the header or footer), the word 'years' occurs 1 time as an plural noun. The part-of-speech tag for a plural noun, NNS, follows the Penn Treebank definition.\n",
    "\n",
    "You can sort of see this as nested information, moving beyond how we might normally work with tabular or spreadsheet data. The blank cells are areas where the data would have normally been duplicated.\n",
    "\n",
    "The HTRC Feature Reader refers to \"pages\" as the $n^{th}$ scanned image of the volume, not the actual number printed on the page. This is why, often, the first page may be the cover, or inside cover, or a blank page.\n",
    "\n",
    "Tokenlists can be retrieved with arguments -- or the stuff that goes inside the `()` -- that combine information by certain dimensions, such as case, POS, or page. For example, `case=False` specified that \"Jaguar\" and \"jaguar\" should be counted together. You may also notice that, by default, only `body` is returned, a default that can be overridden.\n",
    "\n",
    "Look at the following list of commands: can you guess what the output will look like? Try for yourself and observe how the output changes.\n",
    "\n",
    "```python\n",
    "vol.tokenlist(case=False)\n",
    "vol.tokenlist(pos=False)\n",
    "vol.tokenlist(pages=False, case=False, pos=False)\n",
    "vol.tokenlist(section='header')\n",
    "vol.tokenlist(section='group')\n",
    "```\n",
    "\n",
    "Details for these arguments are available in the code documentation for the Feature Reader or by running:\n",
    "\n",
    "```python\n",
    "vol.tokenlist?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with DataFrames\n",
    "\n",
    "The Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier (`tl`), three skills are particularily valuable:\n",
    "\n",
    "1. Selecting subsets by a condition\n",
    "2. Slicing by named row index\n",
    "3. Grouping and aggregating\n",
    "\n",
    "### Selecting Subsets of a DataFrame by a Condition\n",
    "\n",
    "Consider this example: *I only want to look at tokens that occur more than a hundred times in the book.* \n",
    "\n",
    "Remembering that the table-like output from the HTRC Feature Reader is a Pandas DataFrame, the way to pursue this goal is to learn to filter and subset DataFrames. Knowing how to do so is important for working with just the data that you need.\n",
    "\n",
    "To subset individual rows of a DataFrame, you can provide a series of True/False values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned.\n",
    "\n",
    "To see this in context, first load a basic tokenlist without parts-of-speech or individual pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple = vol.tokenlist(pos=False, pages=False)\n",
    "# .sample(5) returns five random words from the full result\n",
    "tl_simple.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select just the relevant tokens, we need to look at each row and evaluate whether it matches the criteria that \"this token has a count greater than 100\". Let's try to convert that requirement to code.\n",
    "\n",
    "\"This token has a count\" means that we are concerned specifically with the 'count' column, which can be singled out from the `tl` table with `tl['count']`. \"greater than 100\" is formalized as `> 100`. Putting it together, try the following and see what you get:\n",
    "\n",
    "```python\n",
    "tl_simple['count'] > 100\n",
    "```\n",
    "\n",
    "It is a DataFrame of True/False values. Each value indicates whether the 'count' column in the corresponding row matches the criteria or not. We haven't selected a subset yet, we simply asked a question and were told for each row when the question was true or false.\n",
    "\n",
    "> You may wonder why section and token are still seen, even though 'count' was selected. These are part of the DataFrame **index**, so they're considered part of the information *about* that row rather than data *in* the row. You can convert the index to data columns with `reset_index()`. In this lesson we will keep the index intact, though there are advanced cases where there are benefits to resetting it.\n",
    "\n",
    "Armed with the True/False values of whether each token's 'count' value is or isn't greater than 100, we can give those values to `tl_simple` in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = tl_simple['count'] > 100\n",
    "tl_simple[matches].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can move the comparison straight into the square brackets, the more conventional equivalent of the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple[tl_simple['count'] > 100].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be expected, many of the tokens that occur very often are common words like \"she\" and \"and\", as well as various punctuation. \n",
    "\n",
    "Multiple conditions can be chained with `&` (and) or `|` (or), using regular brackets so that Python knows the order of operations. For example, words with a count greater than 150 *and* a count less than 200 are selected in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple[(tl_simple['count'] > 150) & (tl_simple['count'] < 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll practice slicing by named row index first.For example, we can add a word between the quotation marks to retrieve only pages where that word occurs. We are using the power of the DataFrame index to retrieve only the rows that match our criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_all = vol.tokenlist(section='all')\n",
    "\n",
    "# add a word between the quotes\n",
    "desert_pages = tl_all.loc[(slice(None), slice(None), 'desert'),]\n",
    "desert_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing DataFrames\n",
    "It is also possible to select a DataFrame subset by specifying the values of its index, a process called **slicing**. For example, you can ask, *\"give me all the verbs for pages 9-12\"*.\n",
    "\n",
    "In the DataFrame returned by `vol.tokenlist()`, page, section, token, and POS were part of the index (try the command `tl.index.names` to confirm). One can think of an index as the margin content of an Excel spreadsheet: the letters along the top and numbers along the left side are the indices. A cell can be referred to as A1, A2, B1... In Pandas, however, you can name these, so instead of A, B, C, or 1,2,3, columns and rows can be referred to by more descriptive names. You can also have multiple levels, so you're not bound by the two-dimensions of a table format. With a multi-indexed DataFrame, you can ask for `Page=24,section=Body, ...`.  One can think of an index as the margin notations in Excel (i.e. 1,2,3... and A,B,C,..), except it can be named and can have multiple levels.\n",
    "\n",
    "Slicing a DataFrame against a labelled index is done using `DataFrame.loc[]`. Try the following examples and see what is returned:\n",
    "\n",
    "- Select information from page 17: \n",
    "  - `tl.loc[(17),]`\n",
    "- Select 'body' section of page 17:\n",
    "  - `tl.loc[(17, 'body'),]`\n",
    "- Select counts of the word 'Euphrates' in the 'body' section of page 17:\n",
    "  - `tl.loc[(17, 'body', 'Euphrates'),]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this code cell to try out the above examples\n",
    "\n",
    "tl.loc[(17),'body','Euphrates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels of the index are specified in order, so in this case the first value refers to 'page', then 'section', and so on. To skip specifying anything for an index level -- that is, to select everything for that level -- `slice(None)` can be used as a placeholder:\n",
    "\n",
    "- Select counts of the word 'Euphrates' for all pages and all page sections\n",
    "  - `tl.loc[(slice(None), slice(None), \"Euphrates\"),]`\n",
    "  \n",
    "Finally, it is possible to select multiple labels for a level of the index, with a list of labels (i.e. `['label1', 'label2']`) or a sequence covering everything from one value to another (i.e. `slice(start, end)`):\n",
    "\n",
    "- Select pages 37, 38, and 52\n",
    "  - `tl.loc[([37, 38, 52]),]`\n",
    "- Select all pages from 37 to 40\n",
    "  - `tl.loc[(slice(37, 40)),]`\n",
    "- Select counts for 'Euphrates' or 'Tigris' from all pages\n",
    "  - `tl.loc[(slice(None), slice(None), [\"Euphrates\", \"Tigris\"]),]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.loc[(slice(None), slice(None), [\"Euphrates\",\"Tigris\"]),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how to slice, let's try to find the sections in this book, and compare where that shows up to the token-per-page pattern previously plotted.\n",
    "\n",
    "The token list we previously set to `tl` only included body text; to include headers and footers in a search, we could generate a tokenlist with `section='all'` specified. Since our workset, the volumes of Josiah Conder's _The Modern Traveller_, does not include chapter headings or breaks, but instead has named sections, as laid out in the table of contents, we'll be recreating a search for chapters by searching for specific page numbers in the header, first by creating a new Data Frame with tokens and metadata for the header section of each page only, then by searching the tokens for the page numbers we pulled from the table of contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_header = vol.tokenlist(section='header')\n",
    "section_breaks = ['1','4','13','19','22','42','47','61','68','79','84','109','123','128',\n",
    "                  '139','149','153','160','175','181','193','201','211','219','224','232',\n",
    "                  '234','237','256','278','283','287','296','302','310','331','358','367']\n",
    "\n",
    "section_break_pages = tl_header.loc[(slice(None), slice(None), section_breaks,),]\n",
    "section_break_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you see any obvious errors or strange results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, token counts were visualized using `tokens.plot()`, a built-in function of DataFrames that uses the Matplotlib visualization library.\n",
    "\n",
    "We can add to the earlier visualization by using Matplotlib directly. Try the following code in a new cell, which goes through each page in the earlier search for page sections, by number, and adds a red vertical line at the place in the chart with matplotlib.pyplot.axvline():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the page numbers from the search for each section\n",
    "page_numbers = section_break_pages.index.get_level_values('page')\n",
    "\n",
    "# Visualize the tokens-per-page from before\n",
    "tokens.plot()\n",
    "\n",
    "# Add vertical lines for pages with section breaks\n",
    "\n",
    "for page_number in page_numbers:\n",
    "    plt.axvline(x=page_number, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Sorting DataFrames \n",
    "A DataFrame can be sorted with `DataFrame.sort_values()`, specifying the column to sort by as the first argument. By default, sorting is done in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple.sort_values('count').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descending order is possible with the argument `ascending=False`, which puts the most common tokens at the top. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple.sort_values('count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequencies and Stopword lists\n",
    "\n",
    "Now we'll look at a look using relative frequencies. Relative frequencies are one way at looking at top words, through their proportional counts. Books have different lengths, so the nominal count of any given word will vary between books, so relative frequencies give us a way to compare two or more books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looking at one volume, let's start to explore the relative frequencies of tokens within the volume. \n",
    "\n",
    "The following cell will display the most common tokens (words or punctuation marks) in a given volume, alongside the number of times they appear. It will also calculate their relative frequencies (found by dividing the number of appearances over the total number of words in the book) and display the results in a DataFrame. The cell may take a few seconds to run because we're looping through every word in the volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokenlist(pos=False, case=False, pages=False).sort_values('count', ascending=False)\n",
    "\n",
    "freqs = []\n",
    "for count in tokens['count']:\n",
    "    freqs.append(count/sum(tokens['count'])) # generating a frequency by volume for a given token \n",
    "                                             # by counting it's frequency in the volume, and dividing \n",
    "                                             # by total tokens in the volume\n",
    "    \n",
    "tokens['rel_frequency'] = freqs\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the most common tokens from the volume and their frequencies. The following cell outputs a bar plot using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we use a command that helps matplotlib plots better display in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Build a list of frequencies and a list of tokens.\n",
    "freqs_1, tokens_1 = [], []\n",
    "for i in range(15):  # top 15 words\n",
    "    freqs_1.append(freqs[i])\n",
    "    tokens_1.append(tokens.index.get_level_values('lowercase')[i])\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(tokens_1))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_1)\n",
    "plt.xticks(x_ticks, tokens_1, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:20] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common tokens are mostly punctuation and basic words that don't provide context. Let's see if we can narrow our search to gain some more relevant insight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of stopwords from the nltk library. Punctuation is in the string library. Let's import nltk and make the stopwords and punctuation accessible to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the nltk library and downloaded the stopwords module, we can look at what is included in each list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english') # making an easier-to-work-with list from the NLTK stopwords\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation) # making a list again, as above\n",
    "\n",
    "print(f\"Stopwords: \\n {stopwords}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Punctuation: \\n {punctuation}\")\n",
    "\n",
    "# Since we imported the stopwords into a basic Python list, we can remove individual words/punctuation using \n",
    "# basic list method of .drop():\n",
    "\n",
    "# stopwords.drop('myself') \n",
    "\n",
    "# punctuation.drop('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these lists of words/characters to ignore in our DataFrame, we can make a few tweaks to our plotting cell to remove the punctuation and display only those words not in our stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_filtered, tokens_filtered, i = [], [], 0\n",
    "\n",
    "while len(tokens_filtered) < 10:\n",
    "    if tokens.index.get_level_values('lowercase')[i] not in stopwords + punctuation:\n",
    "        freqs_filtered.append(freqs[i])\n",
    "        tokens_filtered.append(tokens.index.get_level_values('lowercase')[i])\n",
    "    i += 1\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(freqs_filtered))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_filtered)\n",
    "plt.xticks(x_ticks, tokens_filtered, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:20] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens from all volumes in our set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how word frequencies compare across all the books in our samples. \n",
    "\n",
    "First we'll set-up a few functions. The first finds the most common noun in a volume, with adjustable parameters for minimum length. The second calculates the relative frequency of a token across all volumes in a FeatureReader collection, saving us the time of doing the calculation like in the above cell. Finally, we'll have a visualization function to create a bar plot of relative frequencies for all volumes in our sample, so that we can easily track how word frequencies differ across titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common nouns\n",
    "\n",
    "Let's see what the most common nouns in this work are by word length. To try, add a number to the second code box below.\n",
    "\n",
    "NOTE: `word_length` defaults to 2. e.g. `most_common_noun(fr_novels.first)` returns 'time'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def most_common_noun(vol, word_length=2):   \n",
    "    # Build a table of common nouns\n",
    "    tokens_1 = vol.tokenlist(pages=False, case=False) # create token list DF\n",
    "    nouns_only = tokens_1.loc[(slice(None), slice(None), ['NN']),] # slice token list DF to get DF with only nouns\n",
    "    top_nouns = nouns_only.sort_values('count', ascending=False) # sort the noun DF by highest count\n",
    "\n",
    "    token_index = top_nouns.index.get_level_values('lowercase') # creating a new DF with token (here noun) frequency \n",
    "                                                                # as index\n",
    "    \n",
    "    # Choose the first token at least as long as word_length with non-alphabetical characters\n",
    "    for i in range(max(token_index.shape)):\n",
    "        if (len(token_index[i]) >= word_length):\n",
    "            if(\"'\", \"!\", \",\", \"?\" not in token_index[i]):\n",
    "                return token_index[i]\n",
    "    print('There is no noun of this length')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a number to the parenthesis between the comma , and end parenthesis )\n",
    "most_common_noun(vol, 10)\n",
    "# most_common_noun(fr_mt.first(), 10) # the same as above, but using a FeatureReader collection instead of a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative token frequency\n",
    "Here, the function `frequency()` returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection (which could include one or more volumes).\n",
    "\n",
    "NOTE: `frequency()` returns a dictionary entry of the form `{'word': frequency}`. e.g. `frequency(fr_novels.first(), 'blue')` returns `{'blue': 0.00012}`\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency(vol, word):\n",
    "    t1 = vol.tokenlist(pages=False, pos=False, case=False)\n",
    "    token_index = t1[t1.index.get_level_values(\"lowercase\") == word]\n",
    "    \n",
    "    if len(token_index['count'])==0:\n",
    "        return {word: 0}\n",
    "    \n",
    "    count = token_index['count'][0]\n",
    "    freq = count/sum(t1['count'])\n",
    "    \n",
    "    return {word: float('%.5f' % freq)} # '%.5f' means it'll return a float rounded to 5 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word in the quotes below\n",
    "frequency(vol, 'prince')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "The code below returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "\n",
    "Try adding different words to see their relative frequency in our sample.\n",
    "\n",
    "NOTE: frequencies are given as percentages rather than true ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency_bar_plot(word, FR_col):\n",
    "    freqs, titles = [], []\n",
    "    for vol in FR_col:\n",
    "        title = vol.title\n",
    "        short_title = title[:6] + (title[6:] and '..')\n",
    "        freqs.append(100*frequency(vol, word)[word]) # calling our frequency function\n",
    "        titles.append(short_title)\n",
    "        \n",
    "    # Format and plot the data -- this replicates the code we used outside of the function in our bar chart above\n",
    "    x_ticks = numpy.arange(len(titles))\n",
    "    plt.bar(x_ticks, freqs)\n",
    "    plt.xticks(x_ticks, titles, fontsize=10, rotation=90)\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Volume title', fontsize=12)\n",
    "    plt.title('Frequency of \"' + word + '\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a word to between the quotes\n",
    "frequency_bar_plot('prince', fr_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's interesting, but since all our titles are the same, it's hard to make sense of the results. Let's try plotting relative frequency over time.\n",
    "\n",
    "The code below returns a DataFrame of relative frequencies, volume years, and page counts, along with a scatter plot.\n",
    "\n",
    "NOTE: frequencies are given in percentages rather than true ratios.\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to visualize frequency by volume\n",
    "def frequency_by_vol(query_word, FR_col):\n",
    "    volumes = pandas.DataFrame()\n",
    "    ids, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in FR_col:\n",
    "        ids.append(vol.id)\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['id'], volumes['pages'], volumes['freq'] = ids, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('id')\n",
    "    \n",
    "    fig_plot = volumes.plot('id','freq', scalex=False)\n",
    "    tick_labels = tuple(volumes['id'])\n",
    "    x_max = int(max(plt.xticks()[0]))  # int() to convert numpy.int32 => int\n",
    "    x_min = int(min(plt.xticks()[0]))\n",
    "    plt.xticks(range(x_min, x_max), tick_labels, rotation=90) \n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('HTID', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)\n",
    "\n",
    "# A bonus function that returns and plots frequences for a FR collection by volume year\n",
    "def frequency_by_year(query_word, FR_col):\n",
    "    volumes = pandas.DataFrame()\n",
    "    years, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in FR_col:\n",
    "        years.append(int(vol.year))\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['year'], volumes['pages'], volumes['freq'] = years, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('year')\n",
    "    \n",
    "    # Set plot dimensions and labels\n",
    "    scatter_plot = volumes.plot.scatter('year', 'freq', color='black', s=50, fontsize=12)\n",
    "    plt.ylim(0-numpy.mean(query_freqs), max(query_freqs)+numpy.mean(query_freqs))\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# replace the word between the quotes to try other terms\n",
    "\n",
    "frequency_by_vol('prince', fr_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making use of the structured file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly useful thing about the Extracted Features dataset is that the tokens in the extracted features files are part-of-speech tagged to differentiate homynyms like 'rose', which can be a name, a noun, and a verb.\n",
    "\n",
    "For each page, the data is also divided into a header, body, and footer section so that you can systematically remove headers or footers from your data if you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw the possibiity of drilling down to the part-of-speech tag earlier when we found the most frequently-occuring noun in a volume. Below, we will look for one part of speech in just the body of our volumes.\n",
    "\n",
    "What do you find? Try editing the code to retrieve tokens of another part of speech. Here are the codes in the Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html. \n",
    "\n",
    "The code is set to retrieve only tokens that occured more than 50 times. Can you change the cut-off and see how it effects your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the most frequently occuring adjectives that occured more than 50 times in the data\n",
    "idx = pandas.IndexSlice\n",
    "vol = next(fr_mt.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl.index = tl.index.droplevel(0)\n",
    "adjectives = tl.loc[idx[:,('JJ')],]\n",
    "adj_dfs = [adjectives for vol in fr_mt.volumes()]\n",
    "all_adj = pandas.concat(adj_dfs).groupby(level='token').sum().sort_values('count', ascending=False)[:50]\n",
    "\n",
    "# displays the first 25 rows of the Pandas dataframe of all adjectives\n",
    "all_adj.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More frequency questions\n",
    "Let's combine what we've learned and written to try to answer some specific questions. First, we'll define a number of variables that we will use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1 = fr_mt.first() # the first volume in our FR collection\n",
    "tokens = vol1.tokenlist(pages=False, pos=False, case=False) # a token list created from that volume\n",
    "tokens = tokens.loc['body'] # Only focus on section='body'\n",
    "\n",
    "# removing stop words and non-alphabetical characters, such as punctuation and numbers:\n",
    "subset1 = tokens[~tokens.index.isin(stopwords) & tokens.index.str.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at notable words is through parts-of-speech. e.g. focusing on `NNP` (proper nouns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol1.tokenlist(pages=False, pos=True, case=True).loc['body']\n",
    "proper_nouns_v1 = tokens.loc[(slice(None), ('NNP')),].sort_values('count', ascending=False) # Select NNP and sort\n",
    "proper_nouns_v1['rel_freq'] = proper_nouns_v1['count'] / proper_nouns_v1['count'].sum() # Calculate Relative frequency\n",
    "proper_nouns_v1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of relative frequencies is that it is easy to compare multiple books. First, let's load volume two of  _The Modern Traveller_ to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 = FeatureReader('ef-data/mdp.39015073767751.json.bz2').first()\n",
    "tokens = vol2.tokenlist(pages=False, pos=True, case=True).loc['body']\n",
    "proper_nouns_v2 = tokens.loc[(slice(None), ('NNP')),].sort_values('count', ascending=False)\n",
    "proper_nouns_v2['rel_freq'] = proper_nouns_v2['count'] / proper_nouns_v2['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns_v1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proper_nouns_v2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the index of 'token' and 'pos' is the same, you can just subtract one DataFrame from another and the code will know to align the rows (i.e. subtracting the `(Cape, NNP)` information). We can do this to see which tokens increased in usage from volume 1 to volume 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vols = (proper_nouns_v2 - proper_nouns_v1)\n",
    "compare_vols.sort_values('rel_freq', ascending=False).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the name of minimizing hassle, we've used our _The Modern Traveller_ workset again for this, but it's admittedly not the most interesting workset for this type of question, as each volume is focused on a different region of the world, as shown by 'Cape' and 'Mr.' -- the most frequent proper nouns in volume 1 -- shown here as having the largest drop in frequency when compared to volume 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
